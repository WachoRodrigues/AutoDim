{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, shutil, glob\n",
    "\n",
    "num_validation_file = 1000\n",
    "test_validation_file = 1000\n",
    "File_List = glob.glob('file path_total')\n",
    "list_index = list(np.arange(0, len(File_List), 1))\n",
    "random_index = random.sample(list_index,len(File_List))\n",
    "for i in range(len(random_index)):\n",
    "    random_index_ = random_index[i]\n",
    "    print(random_index_)\n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "    if i < num_validation_file:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Validation' %random_index_)\n",
    "#==========================================================================Test SET==========================================\n",
    "    if num_validation_file <= i and num_validation_file + test_validation_file > i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Test' %random_index_)\n",
    "#==========================================================================Training SET==========================================    \n",
    "    if num_validation_file + test_validation_file <= i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Training' %random_index_)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_1:0\", shape=(38,), dtype=float32)\n",
      "Tensor(\"batch:0\", shape=(1000, 4501), dtype=float32) Tensor(\"batch:1\", shape=(1000, 38), dtype=float32) Tensor(\"batch:2\", shape=(1000, 38), dtype=float32) Tensor(\"batch:3\", shape=(1000, 3), dtype=float32) Tensor(\"batch:4\", shape=(1000, 1), dtype=float32)\n",
      "Tensor(\"add_5:0\", shape=(38,), dtype=float32)\n",
      "Tensor(\"batch_1:0\", shape=(1000, 4501), dtype=float32) Tensor(\"batch_1:1\", shape=(1000, 38), dtype=float32) Tensor(\"batch_1:2\", shape=(1000, 38), dtype=float32) Tensor(\"batch_1:3\", shape=(1000, 3), dtype=float32) Tensor(\"batch_1:4\", shape=(1000, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  \n",
    "\n",
    "learning_rate = 0.001\n",
    "num_files = 6010\n",
    "vld_num_files = 1000\n",
    "X_length = 4511\n",
    "batch_size_ = 1000\n",
    "n_classes = 38\n",
    "epochs = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([('file path_Training' % i) for i in range(num_files)],  \n",
    "                                                    shuffle=True, name='filename_queue')\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    xy_data = tf.stack(xy_data)\n",
    "    y_1=tf.cast(xy_data[-7], tf.int32)\n",
    "    y_2=tf.cast(xy_data[-6], tf.int32)\n",
    "    y_3=tf.cast(xy_data[-5], tf.int32)\n",
    "    y_1=tf.one_hot(y_1, n_classes)\n",
    "    y_2=tf.one_hot(y_2, n_classes)\n",
    "    y_3=tf.one_hot(y_3, n_classes)\n",
    "    y_data = y_1 + y_2 + y_3\n",
    "    y_data_ = y_1*xy_data[-4] + y_2*xy_data[-3] + y_3*xy_data[-2]\n",
    "    y_data = tf.to_float(y_data)\n",
    "    y_data_ = tf.to_float(y_data_)\n",
    "    X_train, y_train,  y_train_, y_train_p, y_train_ind = tf.train.batch([xy_data[:-10], y_data, y_data_, xy_data[-4:-1],\n",
    "                                                                xy_data[-1:]], batch_size = batch_size_)\n",
    "    \n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "\n",
    "    filename_queue_vld = tf.train.string_input_producer([('file path_Validation' % i) for i in range(vld_num_files)], \n",
    "                                                        shuffle=True, name='filename_queue')\n",
    "    reader_vld = tf.TextLineReader()\n",
    "    key, value_vld = reader.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data_vld = tf.decode_csv(value_vld, record_defaults=record_defaults)\n",
    "    xy_data_vld = tf.stack(xy_data_vld)\n",
    "    y_1_vld=tf.cast(xy_data_vld[-7], tf.int32)\n",
    "    y_2_vld=tf.cast(xy_data_vld[-6], tf.int32)\n",
    "    y_3_vld=tf.cast(xy_data_vld[-5], tf.int32)\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes)\n",
    "    y_data_vld = y_1_vld + y_2_vld + y_3_vld\n",
    "    y_data_vld_ = y_1_vld*xy_data_vld[-4] + y_2_vld*xy_data_vld[-3] + y_3_vld*xy_data_vld[-2]\n",
    "    y_data_vld = tf.to_float(y_data_vld)\n",
    "    y_data_vld_ = tf.to_float(y_data_vld_)\n",
    "    X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind = tf.train.batch([xy_data_vld[:-10], y_data_vld, y_data_vld_, xy_data_vld[-4:-1], \n",
    "                                                       xy_data_vld[-1:]], batch_size = batch_size_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4501, 1)\n",
      "(?, 1126, 64)\n",
      "(?, 126, 64)\n",
      "(?, 8064)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.float32, [None, 4501, 1], name = 'inputs')\n",
    "    labels_1 = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_1')\n",
    "    logit_num = tf.placeholder(tf.int32, [None, 3], name = 'logits_Top_3')\n",
    "    label_num = tf.placeholder(tf.int32, [None, 3], name = 'labels_Top_3')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    \n",
    "    #   model architecture(CNN_2)    \n",
    "\"\"\"\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=50, strides=2,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=25, strides=3, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    flat = tf.reshape(max_pool_2, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 500,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"     \n",
    "\n",
    "    #   model architecture(CNN_3)    \n",
    "    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=20, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=2, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_3, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "  \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_2, labels=labels_1))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    correct_pred = tf.equal(logit_num, label_num)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('file path'))\n",
    "    iteration = 1\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for e in range(epochs):\n",
    "        for i in  range(50):    \n",
    "            X_tr, y_tr, y_tr_, y_tr_p, y_ind = sess.run([X_train, y_train, y_train_, y_train_p, y_train_ind])\n",
    "            X_tr= np.reshape(X_tr, (-1, 4501, 1))\n",
    "            feed = {inputs_ : X_tr, labels_1 : y_tr, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            loss, _ , logit = sess.run([cost, optimizer, logits_2], feed_dict = feed)            \n",
    "            y_lab = np.empty([batch_size_, 3])\n",
    "            y_logit = np.empty([batch_size_, 3])\n",
    "            for i in range(batch_size_):\n",
    "                if y_ind[i,0] == 2:\n",
    "                    y_lab[i]=np.argsort(y_tr_[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                elif y_ind[i,0] == 1:\n",
    "                    z=np.argsort(y_tr_[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [0])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [0])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                elif y_ind[i,0] == 0:\n",
    "                    z=np.argsort(y_tr_[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [0,0])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [0,0])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "            feed = {logit_num : y_logit, label_num: y_lab}\n",
    "            acc = sess.run(accuracy, feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)                   \n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                    \"Iteration: {:d}\".format(iteration),\n",
    "                    \"Train loss: {:6f}\".format(loss),\n",
    "                    \"Train acc: {:.6f}\".format(acc))\n",
    "\n",
    "###================================================================ VALIDATION =====================================\n",
    "            if (iteration %5 == 0):\n",
    "                X_vd, y_vd, y_vd_, y_vd_p, y_ind_vd = sess.run([X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind])\n",
    "                X_vd= np.reshape(X_vd, (-1, 4501, 1))\n",
    "                feed = {inputs_ : X_vd, labels_1 : y_vd, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "                loss_vd,  logit_vd = sess.run([cost, logits_2], feed_dict = feed)            \n",
    "                y_lab_vd = np.empty([batch_size_, 3])\n",
    "                y_logit_vd = np.empty([batch_size_, 3])\n",
    "                for i in range(batch_size_):\n",
    "                    if y_ind_vd[i,0] == 2:\n",
    "                        y_lab_vd[i]=np.argsort(y_vd_[i])[-3:]\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        y_logit_vd[i]=np.argsort(logit_vd[i])[-3:]\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                    elif y_ind_vd[i,0] == 1:\n",
    "                        z=np.argsort(y_vd_[i])[-2:]\n",
    "                        y_lab_vd[i]=np.append(z, [0])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-2:]\n",
    "                        y_logit_vd[i]=np.append(z_, [0])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                    elif y_ind_vd[i,0] == 0:\n",
    "                        z=np.argsort(y_vd_[i])[-1:]\n",
    "                        y_lab_vd[i]=np.append(z, [0,0])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-1:]\n",
    "                        y_logit_vd[i]=np.append(z_, [0,0])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                feed = {logit_num : y_logit_vd, label_num: y_lab_vd}\n",
    "                acc_vd = sess.run(accuracy, feed_dict = feed)\n",
    "                validation_acc.append(acc_vd)\n",
    "                validation_loss.append(loss_vd)           \n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Validation loss: {:6f}\".format(loss_vd),\n",
    "                        \"Validation acc: {:.6f}\".format(acc_vd))                \n",
    "            iteration += 1 \n",
    "        saver.save(sess,\"file path\")\n",
    "coord.request_stop()\n",
    "coord.join(threads) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
